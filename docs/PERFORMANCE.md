# Sifaka Performance Guide

This guide covers performance optimization, benchmarks, and best practices for production deployments.

## üéØ Performance Overview

Sifaka is designed for production use with several performance optimizations:

- **‚ö° Parallel Processing**: Validators and critics run simultaneously
- **üöÄ Async Throughout**: Full async/await support for non-blocking operations
- **üíæ Smart Caching**: Model responses and validation results cached
- **üîÑ Efficient Iteration**: Minimal overhead between improvement rounds
- **üìä Configurable Limits**: Control resource usage with iteration limits

## üìä Benchmark Results

### Single Improvement Task

**Test Setup**: 
- Prompt: "Write about renewable energy benefits" (500-word target)
- Model: OpenAI GPT-4
- Validators: Length (min 400, max 600 words) + Content (required terms)
- Critics: Reflexion + Constitutional
- Hardware: MacBook Pro M2, 16GB RAM

| Metric | Value | Notes |
|--------|-------|-------|
| **Total Time** | 8.2s | End-to-end completion |
| **Iterations** | 2.3 avg | Usually 2-3 rounds needed |
| **Model Calls** | 4.6 avg | Generation + critic calls |
| **Tokens Used** | ~3,200 | Input + output tokens |
| **Success Rate** | 94% | Meets all validation criteria |

### Performance by Component

| Component | Time (ms) | % of Total | Parallelizable |
|-----------|-----------|------------|----------------|
| **Text Generation** | 3,200 | 39% | ‚ùå Sequential |
| **Validation** | 150 | 2% | ‚úÖ Parallel |
| **Critique** | 2,800 | 34% | ‚úÖ Parallel |
| **Feedback Processing** | 50 | 1% | ‚ùå Sequential |
| **Network/API** | 2,000 | 24% | ‚ö° Cached |

### Scaling Characteristics

```mermaid
graph LR
    subgraph "Validators"
        V1[Validator 1<br/>~50ms]
        V2[Validator 2<br/>~75ms]
        V3[Validator 3<br/>~100ms]
    end
    
    subgraph "Critics"
        C1[Critic 1<br/>~1.2s]
        C2[Critic 2<br/>~1.5s]
    end
    
    subgraph "Results"
        VMAX[Max: 100ms<br/>Not 225ms]
        CMAX[Max: 1.5s<br/>Not 2.7s]
    end
    
    V1 --> VMAX
    V2 --> VMAX
    V3 --> VMAX
    
    C1 --> CMAX
    C2 --> CMAX
    
    classDef component fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef result fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    
    class V1,V2,V3,C1,C2 component
    class VMAX,CMAX result
```

**Key Insight**: Adding more validators/critics increases capability without proportional time increase due to parallel processing.

## ‚ö° Optimization Strategies

### 1. Model Selection

Different models have different performance characteristics:

| Model | Speed | Quality | Cost | Best For |
|-------|-------|---------|------|----------|
| **GPT-4o-mini** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üí∞ | Development, simple tasks |
| **GPT-4** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞üí∞ | Production, complex tasks |
| **Claude-3-Haiku** | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | üí∞üí∞ | Fast, high-quality |
| **Groq Llama-3.1** | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | üí∞ | Ultra-fast inference |

**Recommendation**: Use fast models for critics, high-quality models for generation.

```python
# Optimized model configuration
dependencies = SifakaDependencies(
    generator="openai:gpt-4",           # High quality for generation
    critics={
        "reflexion": "groq:llama-3.1-8b-instant",  # Fast for critique
        "constitutional": "anthropic:claude-3-haiku"  # Balanced
    }
)
```

### 2. Iteration Limits

Control resource usage with smart iteration limits:

```python
# Conservative (fast, good enough)
config = SifakaConfig(max_iterations=2)  # ~4-6 seconds

# Balanced (default)
config = SifakaConfig(max_iterations=3)  # ~6-10 seconds

# Thorough (best quality)
config = SifakaConfig(max_iterations=5)  # ~10-20 seconds
```

### 3. Validator Optimization

Order validators by speed for early termination:

```python
validators = [
    LengthValidator(min_length=100),        # ~10ms - check first
    ContentValidator(required=["AI"]),      # ~50ms - check second
    SentimentValidator(target="positive"),  # ~200ms - check last
]
```

### 4. Caching Strategies

Sifaka includes multiple caching layers:

```python
# Enable classifier caching (recommended)
sentiment_validator = sentiment_validator(cached=True)

# Use persistent storage for thought caching
engine = SifakaEngine(
    persistence=SifakaFilePersistence("./cache")
)
```

## üöÄ Production Optimizations

### 1. Async Batch Processing

Process multiple improvements concurrently:

```python
import asyncio

async def batch_improve(prompts):
    tasks = [sifaka.improve(prompt) for prompt in prompts]
    return await asyncio.gather(*tasks)

# Process 10 prompts concurrently
results = await batch_improve([
    "Write about AI ethics",
    "Explain quantum computing", 
    # ... 8 more prompts
])
```

**Performance**: ~10x faster than sequential processing for batch jobs.

### 2. Connection Pooling

For high-throughput applications:

```python
# Configure HTTP client with connection pooling
import httpx

async with httpx.AsyncClient(
    limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
) as client:
    # Use client for all API calls
    pass
```

### 3. Resource Monitoring

Monitor resource usage in production:

```python
from sifaka.utils.thought_inspector import get_thought_overview

result = await sifaka.improve("Your prompt")
overview = get_thought_overview(result)

print(f"Iterations: {overview['total_iterations']}")
print(f"Model calls: {overview['total_model_calls']}")
print(f"Processing time: {overview['total_processing_time_ms']}ms")
```

## üìà Scaling Patterns

### Horizontal Scaling

```mermaid
graph TD
    subgraph "Load Balancer"
        LB[Load Balancer]
    end
    
    subgraph "Sifaka Instances"
        S1[Sifaka Instance 1]
        S2[Sifaka Instance 2]
        S3[Sifaka Instance 3]
    end
    
    subgraph "Shared Resources"
        REDIS[Redis Cache]
        POSTGRES[PostgreSQL Storage]
    end
    
    LB --> S1
    LB --> S2
    LB --> S3
    
    S1 --> REDIS
    S2 --> REDIS
    S3 --> REDIS
    
    S1 --> POSTGRES
    S2 --> POSTGRES
    S3 --> POSTGRES
    
    classDef instance fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    classDef shared fill:#e8f5e8,stroke:#388e3c,stroke-width:2px
    
    class S1,S2,S3 instance
    class REDIS,POSTGRES shared
```

### Vertical Scaling

| Resource | Impact | Recommendation |
|----------|--------|----------------|
| **CPU** | Medium | 4-8 cores sufficient |
| **Memory** | High | 8GB+ for caching |
| **Network** | High | Low latency to APIs |
| **Storage** | Low | SSD for file persistence |

## üîß Configuration Tuning

### Development Environment

```python
# Fast iteration for development
config = SifakaConfig(
    model="openai:gpt-4o-mini",  # Fastest model
    max_iterations=2,            # Quick results
    critics=["reflexion"]        # Single critic
)
```

### Production Environment

```python
# Balanced performance and quality
config = SifakaConfig(
    model="openai:gpt-4",                    # High quality
    max_iterations=3,                        # Reasonable limit
    critics=["reflexion", "constitutional"]  # Multiple critics
)

# With hybrid storage for performance
storage = FlexibleHybridPersistence([
    BackendConfig(MemoryPersistence(), role=BackendRole.CACHE),
    BackendConfig(RedisPersistence(), role=BackendRole.PRIMARY),
    BackendConfig(SifakaFilePersistence(), role=BackendRole.BACKUP)
])

engine = SifakaEngine(config=config, persistence=storage)
```

### High-Throughput Environment

```python
# Optimized for maximum throughput
config = SifakaConfig(
    model="groq:llama-3.1-8b-instant",  # Fastest inference
    max_iterations=2,                    # Minimal iterations
    critics=["reflexion"]                # Single fast critic
)
```

## üìä Performance Monitoring

### Key Metrics to Track

1. **Latency Metrics**
   - End-to-end response time
   - Per-iteration time
   - Model API response time

2. **Quality Metrics**
   - Validation success rate
   - Average iterations needed
   - User satisfaction scores

3. **Resource Metrics**
   - Token usage per request
   - Memory usage
   - API rate limit utilization

### Monitoring Code

```python
import time
from sifaka.utils.thought_inspector import get_thought_overview

async def monitored_improve(prompt):
    start_time = time.time()
    
    result = await sifaka.improve(prompt)
    
    end_time = time.time()
    overview = get_thought_overview(result)
    
    # Log metrics
    metrics = {
        "total_time": end_time - start_time,
        "iterations": overview["total_iterations"],
        "model_calls": overview["total_model_calls"],
        "success": result.validation_passed(),
        "tokens": overview.get("total_tokens", 0)
    }
    
    # Send to monitoring system
    log_metrics(metrics)
    
    return result
```

## üéØ Performance Best Practices

### Do's ‚úÖ

1. **Use parallel processing** - Let validators and critics run simultaneously
2. **Cache aggressively** - Enable caching for classifiers and storage
3. **Choose models wisely** - Fast models for critics, quality models for generation
4. **Set reasonable limits** - Don't let iterations run indefinitely
5. **Monitor in production** - Track latency, quality, and resource usage
6. **Batch when possible** - Process multiple requests concurrently

### Don'ts ‚ùå

1. **Don't use slow models everywhere** - Reserve GPT-4 for generation
2. **Don't ignore caching** - Caching provides massive speedups
3. **Don't set unlimited iterations** - Always have reasonable limits
4. **Don't run validators sequentially** - They're designed to run in parallel
5. **Don't ignore error handling** - Network issues will happen
6. **Don't skip monitoring** - You can't optimize what you don't measure

## üîÆ Future Performance Improvements

Planned optimizations:
- **Streaming responses** - Real-time feedback during generation
- **Semantic caching** - Cache based on prompt similarity
- **Model routing** - Automatic model selection based on task
- **Distributed processing** - Scale across multiple machines
- **Edge deployment** - Run validators locally for speed

With these optimizations, Sifaka delivers production-ready performance while maintaining the quality guarantees that make it unique.
