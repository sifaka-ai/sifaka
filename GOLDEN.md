# Sifaka State Container Design

## Current Architecture Overview

The Sifaka framework currently orchestrates text generation, validation, and improvement through the `Chain` class, which coordinates:

1. **Models**: Generate text from prompts
2. **Validators**: Check if text meets specific criteria
3. **Critics**: Improve text quality through various techniques
4. **Retrievers**: Fetch relevant information for queries (currently used primarily by critics)

The current flow is:
1. Generate text using a model and prompt
2. Validate the generated text
3. If validation fails and `apply_improvers_on_validation_failure` is enabled, use critics to improve the text
4. Re-validate the improved text
5. Repeat steps 3-4 until validation passes or max iterations is reached

## Implementation Approach

> **Note**: This design focuses on a clean rewrite without backward compatibility concerns. We'll implement the new architecture first and consider backward compatibility later if needed.

## Proposed Enhancements

### 1. Make Retrievers Available to Models

Currently, retrievers are primarily used by critics (especially retrieval-augmented critics like Self-RAG). Making retrievers available to models would allow for retrieval-augmented generation at the initial text generation stage, not just during improvement.

### 2. State Container Design

#### Naming Considerations

For the central state container, we have several naming options to consider:

- **Thought**: Represents the evolving state of the generation process as a cognitive process
- **Assertion**: Emphasizes the factual nature of the generated content
- **Discussion**: Highlights the iterative, conversational nature of the process
- **Context**: Focuses on the container's role in maintaining context throughout the chain
- **State**: A more technical, straightforward description of its purpose

For now, we'll use "Thought" as a working name, but this can be revisited.

#### Container Design

The state container would track the complete context throughout the chain execution, including:

1. **Initial prompt**: The original prompt provided to the chain
2. **Retrieved context**: Documents retrieved based on the prompt (before generation)
3. **Generated text**: The text generated by the model
4. **Validation results**: Results from all validators with detailed metadata
5. **Critic feedback**: Feedback from critics on how to improve the text
6. **Iteration history**: Record of all iterations through the chain

## Thought Container Design

```python
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union
import copy
import json
import os
import time

from sifaka.results import ValidationResult, ImprovementResult


@dataclass
class Thought:
    """Container for the complete state of a generation process.

    This class represents the evolving state of text generation, validation,
    and improvement throughout the chain execution process. It maintains
    the complete context needed by each component in the chain.

    Attributes:
        prompt: The original prompt provided to the chain
        retrieved_context: Documents retrieved based on the prompt
        generated_text: The text generated by the model
        validation_results: Results from all validators
        critic_feedback: Feedback from critics on how to improve the text
        improvement_results: Results from all improvers
        iteration: The current iteration number
        history: Record of previous states in the chain execution
        metadata: Additional metadata about the execution
    """

    prompt: str
    retrieved_context: Optional[List[str]] = None
    generated_text: Optional[str] = None
    validation_results: List[ValidationResult] = field(default_factory=list)
    critic_feedback: Optional[str] = None
    improvement_results: List[ImprovementResult] = field(default_factory=list)
    iteration: int = 0
    history: List["Thought"] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def add_to_history(self) -> None:
        """Add the current state to the history."""
        # Create a copy of the current state without history to avoid recursion
        current_state = copy.deepcopy(self)
        current_state.history = []
        self.history.append(current_state)

    def next_iteration(self) -> "Thought":
        """Create a new Thought for the next iteration."""
        self.add_to_history()
        return Thought(
            prompt=self.prompt,
            retrieved_context=self.retrieved_context,
            generated_text=self.generated_text,
            iteration=self.iteration + 1,
            history=self.history.copy(),
            metadata=self.metadata.copy()
        )

    def all_validation_issues(self) -> List[str]:
        """Get all validation issues from the current state."""
        issues = []
        for result in self.validation_results:
            if result.issues:
                issues.extend(result.issues)
        return issues

    def all_validation_suggestions(self) -> List[str]:
        """Get all validation suggestions from the current state."""
        suggestions = []
        for result in self.validation_results:
            if result.suggestions:
                suggestions.extend(result.suggestions)
        return suggestions

    def validation_passed(self) -> bool:
        """Check if all validations passed."""
        return all(result.passed for result in self.validation_results)

    def to_dict(self) -> Dict[str, Any]:
        """Convert the Thought to a dictionary for serialization."""
        # Create a copy without history to avoid recursion
        thought_dict = {
            "prompt": self.prompt,
            "retrieved_context": self.retrieved_context,
            "generated_text": self.generated_text,
            "validation_results": [self._validation_result_to_dict(r) for r in self.validation_results],
            "critic_feedback": self.critic_feedback,
            "improvement_results": [self._improvement_result_to_dict(r) for r in self.improvement_results],
            "iteration": self.iteration,
            "metadata": self.metadata,
            # Convert history separately
            "history": [self._history_item_to_dict(h) for h in self.history]
        }
        return thought_dict

    @staticmethod
    def _validation_result_to_dict(result: ValidationResult) -> Dict[str, Any]:
        """Convert a ValidationResult to a dictionary."""
        return {
            "passed": result.passed,
            "message": result.message,
            "score": result.score,
            "issues": result.issues,
            "suggestions": result.suggestions,
            "details": result.details
        }

    @staticmethod
    def _improvement_result_to_dict(result: ImprovementResult) -> Dict[str, Any]:
        """Convert an ImprovementResult to a dictionary."""
        return {
            "original_text": result.original_text,
            "improved_text": result.improved_text,
            "changes_made": result.changes_made,
            "message": result.message,
            "details": result.details,
            "processing_time_ms": result.processing_time_ms
        }

    def _history_item_to_dict(self, history_item: "Thought") -> Dict[str, Any]:
        """Convert a history item to a dictionary."""
        # Don't include nested history to avoid recursion
        return {
            "prompt": history_item.prompt,
            "retrieved_context": history_item.retrieved_context,
            "generated_text": history_item.generated_text,
            "validation_results": [self._validation_result_to_dict(r) for r in history_item.validation_results],
            "critic_feedback": history_item.critic_feedback,
            "improvement_results": [self._improvement_result_to_dict(r) for r in history_item.improvement_results],
            "iteration": history_item.iteration,
            "metadata": history_item.metadata,
            "history": []  # Empty history to avoid recursion
        }

    def save_to_file(self, file_path: str) -> None:
        """Save the Thought to a JSON file."""
        with open(file_path, "w") as f:
            json.dump(self.to_dict(), f, indent=2)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "Thought":
        """Create a Thought from a dictionary."""
        # Create the base Thought without history
        thought = cls(
            prompt=data["prompt"],
            retrieved_context=data.get("retrieved_context"),
            generated_text=data.get("generated_text"),
            validation_results=[cls._dict_to_validation_result(r) for r in data.get("validation_results", [])],
            critic_feedback=data.get("critic_feedback"),
            improvement_results=[cls._dict_to_improvement_result(r) for r in data.get("improvement_results", [])],
            iteration=data.get("iteration", 0),
            metadata=data.get("metadata", {})
        )

        # Add history items
        for history_item in data.get("history", []):
            history_thought = cls.from_dict(history_item)
            thought.history.append(history_thought)

        return thought

    @staticmethod
    def _dict_to_validation_result(data: Dict[str, Any]) -> ValidationResult:
        """Create a ValidationResult from a dictionary."""
        return ValidationResult(
            passed=data["passed"],
            message=data.get("message", ""),
            score=data.get("score"),
            issues=data.get("issues"),
            suggestions=data.get("suggestions"),
            _details=data.get("details", {})
        )

    @staticmethod
    def _dict_to_improvement_result(data: Dict[str, Any]) -> ImprovementResult:
        """Create an ImprovementResult from a dictionary."""
        return ImprovementResult(
            _original_text=data["original_text"],
            _improved_text=data["improved_text"],
            _changes_made=data["changes_made"],
            message=data.get("message", ""),
            _details=data.get("details", {}),
            processing_time_ms=data.get("processing_time_ms")
        )

    @classmethod
    def load_from_file(cls, file_path: str) -> "Thought":
        """Load a Thought from a JSON file."""
        with open(file_path, "r") as f:
            data = json.load(f)
        return cls.from_dict(data)
```

## Detailed Implementation Plan

### Phase 1: Create the Thought Container

1. Create a new module `sifaka/state.py` with the `Thought` class as defined above
2. Add unit tests for the `Thought` class in `tests/test_state.py`
3. Create utility functions for working with `Thought` objects:
   - `create_thought_from_prompt(prompt: str) -> Thought`
   - `create_thought_from_result(result: Result) -> Thought`

```python
# Example test case for Thought serialization/deserialization
def test_thought_serialization():
    """Test that a Thought can be serialized and deserialized."""
    # Create a thought
    thought = Thought(
        prompt="Write a short story about a robot.",
        retrieved_context=["Robots are machines that can be programmed to perform tasks."],
        generated_text="Once upon a time, there was a robot named R2D2...",
        validation_results=[
            ValidationResult(
                passed=False,
                message="Text is too short",
                issues=["Text has only 30 words, minimum required is 50"],
                suggestions=["Add more details about the robot's adventures"]
            )
        ],
        critic_feedback="The story needs more character development and a clearer plot.",
        iteration=1
    )

    # Serialize to JSON
    thought_dict = thought.to_dict()

    # Deserialize from JSON
    thought2 = Thought.from_dict(thought_dict)

    # Check that the deserialized thought matches the original
    assert thought2.prompt == thought.prompt
    assert thought2.retrieved_context == thought.retrieved_context
    assert thought2.generated_text == thought.generated_text
    assert len(thought2.validation_results) == len(thought.validation_results)
    assert thought2.validation_results[0].passed == thought.validation_results[0].passed
    assert thought2.critic_feedback == thought.critic_feedback
    assert thought2.iteration == thought.iteration
```

### Phase 2: Create RetrievalAugmentedModel Wrapper

1. Create a new class `RetrievalAugmentedModel` in `sifaka/models/retrieval_augmented.py`:

```python
class RetrievalAugmentedModel:
    """Model wrapper that augments generation with retrieval.

    This class wraps a model and a retriever to provide retrieval-augmented
    generation capabilities. It retrieves relevant documents for the prompt
    and includes them in the context before generating text.

    Attributes:
        model: The underlying model to use for generation.
        retriever: The retriever to use for retrieving documents.
        max_passages: Maximum number of passages to retrieve.
        include_passages_in_prompt: Whether to include retrieved passages in the prompt.
        passage_prefix: Prefix to add before each passage in the prompt.
    """

    def __init__(
        self,
        model: Model,
        retriever: Retriever,
        max_passages: int = 5,
        include_passages_in_prompt: bool = True,
        passage_prefix: str = "Relevant information:\n",
    ):
        """Initialize the retrieval-augmented model.

        Args:
            model: The underlying model to use for generation.
            retriever: The retriever to use for retrieving documents.
            max_passages: Maximum number of passages to retrieve.
            include_passages_in_prompt: Whether to include retrieved passages in the prompt.
            passage_prefix: Prefix to add before each passage in the prompt.
        """
        self.model = model
        self.retriever = retriever
        self.max_passages = max_passages
        self.include_passages_in_prompt = include_passages_in_prompt
        self.passage_prefix = passage_prefix

    def generate(self, prompt: str, **options: Any) -> str:
        """Generate text with retrieval augmentation.

        Args:
            prompt: The prompt to generate text for.
            **options: Additional options to pass to the model.

        Returns:
            The generated text.
        """
        # Retrieve relevant passages
        passages = self.retriever.retrieve(prompt)[:self.max_passages]

        # Augment the prompt with retrieved passages if enabled
        if self.include_passages_in_prompt and passages:
            augmented_prompt = f"{prompt}\n\n{self.passage_prefix}\n"
            for i, passage in enumerate(passages):
                augmented_prompt += f"[{i+1}] {passage}\n\n"
        else:
            augmented_prompt = prompt

        # Generate text using the augmented prompt
        return self.model.generate(augmented_prompt, **options)

    def count_tokens(self, text: str) -> int:
        """Count tokens in text using the underlying model.

        Args:
            text: The text to count tokens for.

        Returns:
            The number of tokens in the text.
        """
        return self.model.count_tokens(text)
```

2. Add factory function in `sifaka/models/factory.py`:

```python
def create_retrieval_augmented_model(
    model: Union[str, Model],
    retriever: Union[str, Retriever],
    **options: Any
) -> RetrievalAugmentedModel:
    """Create a retrieval-augmented model.

    Args:
        model: The model to use for generation, either a Model instance or a string.
        retriever: The retriever to use, either a Retriever instance or a string.
        **options: Additional options for the retrieval-augmented model.

    Returns:
        A RetrievalAugmentedModel instance.
    """
    # Resolve model if it's a string
    if isinstance(model, str):
        model = create_model(model)

    # Resolve retriever if it's a string
    if isinstance(retriever, str):
        retriever = create_retriever(retriever)

    return RetrievalAugmentedModel(model=model, retriever=retriever, **options)
```

### Phase 3: Modify Chain to Use Thought Container

1. Update the `Chain` class to use the `Thought` container internally:

```python
class Chain:
    # ... existing code ...

    def __init__(
        self,
        config: Optional[SifakaConfig] = None,
        model_factory: Optional[Callable[[str, str], Model]] = None,
        max_improvement_iterations: int = 3,
    ):
        # ... existing initialization ...

        # Add retriever attribute
        self._retriever: Optional[Retriever] = None

    def with_retriever(self, retriever: Union[str, Retriever], **options: Any) -> "Chain":
        """Configure the chain with a retriever for retrieval-augmented generation.

        Args:
            retriever: The retriever to use, either a Retriever instance or a string.
            **options: Additional options for the retriever.

        Returns:
            The chain instance for method chaining.
        """
        # Resolve retriever if it's a string
        if isinstance(retriever, str):
            from sifaka.retrievers.factory import create_retriever
            self._retriever = create_retriever(retriever, **options)
        else:
            self._retriever = retriever

        return self

    def run(self) -> Result:
        """Execute the chain and return the result.

        This method runs the complete chain execution process with a feedback loop:
        1. Creates a Thought container to track the state
        2. Retrieves relevant documents if a retriever is configured
        3. Generates text using the model and prompt (with retrieved context if available)
        4. Validates the generated text using all configured validators
        5. If validation fails and apply_improvers_on_validation_failure is True:
           a. Gets feedback from critics on how to improve the text
           b. Sends the original text + feedback to the model to generate improved text
           c. Re-validates the improved text
           d. Repeats steps 5a-5c until validation passes or max iterations is reached
        6. If validation passes and the text needs improvement:
           a. Improves the text using configured improvers
        7. Returns a Result object with the final text and all validation/improvement results

        Returns:
            A Result object containing the final text, validation results, and improvement results
        """
        import logging
        import time

        from sifaka.state import Thought
        from sifaka.utils.error_handling import chain_context, log_error

        logger = logging.getLogger(__name__)

        # Start timing
        start_time = time.time()

        try:
            # Check configuration
            if not self._model:
                raise ChainError(
                    message="Model not specified",
                    component="Chain",
                    operation="run",
                    suggestions=["Use with_model() to specify a model before running the chain"],
                )

            if not self._prompt:
                raise ChainError(
                    message="Prompt not specified",
                    component="Chain",
                    operation="run",
                    suggestions=["Use with_prompt() to specify a prompt before running the chain"],
                )

            # Create initial Thought container
            thought = Thought(prompt=self._prompt)

            # Retrieve relevant documents if a retriever is configured
            if self._retriever:
                logger.debug("Retrieving relevant documents for prompt")
                thought.retrieved_context = self._retriever.retrieve(self._prompt)
                logger.debug(f"Retrieved {len(thought.retrieved_context)} documents")

            # Generate text
            logger.debug("Generating text")
            if thought.retrieved_context and self._options.get("use_retrieval_for_generation", True):
                # Create augmented prompt with retrieved context
                augmented_prompt = self._create_augmented_prompt(self._prompt, thought.retrieved_context)
                text = self._generate_text(augmented_prompt)
            else:
                text = self._generate_text(self._prompt)

            thought.generated_text = text
            logger.debug(f"Generated text ({len(text)} chars)")

            # Initialize results lists
            from sifaka.results import ImprovementResult as ResultsImprovementResult
            from sifaka.results import ValidationResult as ResultsValidationResult

            validation_results: List[ResultsValidationResult] = []
            improvement_results: List[ResultsImprovementResult] = []

            # Main improvement loop
            current_text = text

            for iteration in range(self._max_improvement_iterations):
                logger.debug(
                    f"Starting improvement iteration {iteration+1}/{self._max_improvement_iterations}"
                )

                # Update thought for this iteration
                if iteration > 0:
                    thought = thought.next_iteration()

                # Validate current text
                validation_passed, validation_results, validation_feedback = self._validate_text(
                    current_text
                )

                # Update thought with validation results
                thought.validation_results = validation_results

                # ... rest of the existing run method ...

            # ... rest of the existing run method ...

        except Exception as e:
            # ... existing error handling ...

    def _create_augmented_prompt(self, prompt: str, retrieved_context: List[str]) -> str:
        """Create an augmented prompt with retrieved context.

        Args:
            prompt: The original prompt.
            retrieved_context: The retrieved documents to include in the prompt.

        Returns:
            The augmented prompt with retrieved context.
        """
        # Get retrieval options from config
        retrieval_prefix = self._options.get("retrieval_prefix", "Relevant information:\n")

        # Create augmented prompt
        augmented_prompt = f"{prompt}\n\n{retrieval_prefix}\n"
        for i, document in enumerate(retrieved_context):
            augmented_prompt += f"[{i+1}] {document}\n\n"

        return augmented_prompt
```

### Phase 4: Update Components to Use Thought Container

1. Update the `Model` interface to accept `Thought` objects:

```python
class ModelWithThought(Protocol):
    """Protocol for models that support the Thought container.

    This protocol extends the Model protocol to support generating text
    using a Thought container, which provides complete context about
    the generation process.
    """

    def generate_with_thought(self, thought: Thought, **options: Any) -> Thought:
        """Generate text using a Thought container.

        Args:
            thought: The Thought container with the prompt and context.
            **options: Additional options for text generation.

        Returns:
            The updated Thought container with generated text.
        """
        ...
```

2. Create adapter classes for existing models:

```python
class ModelAdapter:
    """Adapter for models to support the Thought container.

    This class adapts existing models to support the Thought container
    by implementing the ModelWithThought protocol.
    """

    def __init__(self, model: Model):
        """Initialize the model adapter.

        Args:
            model: The underlying model to adapt.
        """
        self.model = model

    def generate_with_thought(self, thought: Thought, **options: Any) -> Thought:
        """Generate text using a Thought container.

        Args:
            thought: The Thought container with the prompt and context.
            **options: Additional options for text generation.

        Returns:
            The updated Thought container with generated text.
        """
        # Create augmented prompt if retrieved context is available
        if thought.retrieved_context:
            # Create augmented prompt with retrieved context
            augmented_prompt = self._create_augmented_prompt(thought.prompt, thought.retrieved_context)
            text = self.model.generate(augmented_prompt, **options)
        else:
            text = self.model.generate(thought.prompt, **options)

        # Update thought with generated text
        thought.generated_text = text

        return thought

    def _create_augmented_prompt(self, prompt: str, retrieved_context: List[str]) -> str:
        """Create an augmented prompt with retrieved context.

        Args:
            prompt: The original prompt.
            retrieved_context: The retrieved documents to include in the prompt.

        Returns:
            The augmented prompt with retrieved context.
        """
        # Create augmented prompt
        augmented_prompt = f"{prompt}\n\nRelevant information:\n"
        for i, document in enumerate(retrieved_context):
            augmented_prompt += f"[{i+1}] {document}\n\n"

        return augmented_prompt
```

## Advanced Memory Persistence with Vector Databases

The Thought container design allows for flexible memory persistence strategies. While the basic implementation uses JSON files for persistence, more advanced use cases would benefit from using vector databases or caching systems.

### Redis-Based Memory Store

Redis can be used as a fast, in-memory storage solution for Thoughts with several advantages:

1. **Fast Access**: Redis provides sub-millisecond response times
2. **Data Structures**: Redis supports complex data structures like lists, sets, and hashes
3. **Expiration**: Thoughts can be set to expire after a certain time
4. **Pub/Sub**: Enables real-time notifications when new Thoughts are created

```python
class RedisThoughtStore:
    """Redis-based storage for Thoughts."""

    def __init__(self, redis_url: str = "redis://localhost:6379/0"):
        """Initialize the Redis thought store.

        Args:
            redis_url: The URL of the Redis server.
        """
        import redis
        self.redis = redis.from_url(redis_url)

    def save(self, thought: Thought, key: Optional[str] = None) -> str:
        """Save a thought to Redis.

        Args:
            thought: The thought to save.
            key: Optional key to use for the thought. If not provided, a UUID is generated.

        Returns:
            The key used to store the thought.
        """
        import uuid

        # Generate a key if not provided
        if key is None:
            key = f"thought:{uuid.uuid4()}"

        # Convert thought to JSON
        thought_json = json.dumps(thought.to_dict())

        # Store in Redis
        self.redis.set(key, thought_json)

        # Add to index
        self.redis.sadd("thoughts", key)

        return key

    def load(self, key: str) -> Optional[Thought]:
        """Load a thought from Redis.

        Args:
            key: The key of the thought to load.

        Returns:
            The loaded thought, or None if not found.
        """
        # Get from Redis
        thought_json = self.redis.get(key)

        if thought_json is None:
            return None

        # Convert from JSON
        thought_dict = json.loads(thought_json)

        # Create thought
        return Thought.from_dict(thought_dict)

    def list_keys(self) -> List[str]:
        """List all thought keys in Redis.

        Returns:
            A list of all thought keys.
        """
        return list(self.redis.smembers("thoughts"))
```

### Milvus/Elasticsearch for Semantic Search

For more advanced memory retrieval based on semantic similarity, vector databases like Milvus or Elasticsearch can be used:

1. **Semantic Search**: Find similar Thoughts based on content meaning, not just keywords
2. **Scalability**: Handle millions of Thoughts efficiently
3. **Filtering**: Combine vector search with metadata filtering
4. **Hybrid Search**: Mix keyword and semantic search for better results

```python
class VectorThoughtStore:
    """Vector database storage for Thoughts with semantic search."""

    def __init__(
        self,
        embedding_model: str = "openai:text-embedding-ada-002",
        vector_db: str = "milvus",
        connection_params: Dict[str, Any] = None
    ):
        """Initialize the vector thought store.

        Args:
            embedding_model: The model to use for generating embeddings.
            vector_db: The vector database to use ("milvus" or "elasticsearch").
            connection_params: Connection parameters for the vector database.
        """
        self.embedding_model = self._create_embedding_model(embedding_model)
        self.vector_db = self._create_vector_db(vector_db, connection_params or {})
        self.collection_name = "thoughts"

        # Ensure collection exists
        self._ensure_collection_exists()

    def _create_embedding_model(self, model_name: str):
        """Create an embedding model."""
        # Implementation depends on the embedding model used
        pass

    def _create_vector_db(self, db_type: str, params: Dict[str, Any]):
        """Create a vector database client."""
        if db_type == "milvus":
            from pymilvus import connections, Collection
            connections.connect(**params)
            return {"type": "milvus", "client": None}  # Milvus uses global connections
        elif db_type == "elasticsearch":
            from elasticsearch import Elasticsearch
            return {"type": "elasticsearch", "client": Elasticsearch(**params)}
        else:
            raise ValueError(f"Unsupported vector database: {db_type}")

    def _ensure_collection_exists(self):
        """Ensure the thoughts collection exists."""
        # Implementation depends on the vector database used
        pass

    def save(self, thought: Thought) -> str:
        """Save a thought to the vector database.

        Args:
            thought: The thought to save.

        Returns:
            The ID of the saved thought.
        """
        # Generate embedding for the thought
        text_to_embed = f"{thought.prompt} {thought.generated_text or ''}"
        embedding = self.embedding_model.embed(text_to_embed)

        # Convert thought to dictionary
        thought_dict = thought.to_dict()

        # Add to vector database
        if self.vector_db["type"] == "milvus":
            from pymilvus import Collection
            collection = Collection(self.collection_name)
            result = collection.insert([
                [thought_dict["id"]],  # ID field
                [embedding],  # Vector field
                [json.dumps(thought_dict)]  # JSON field
            ])
            return str(result.primary_keys[0])
        elif self.vector_db["type"] == "elasticsearch":
            es = self.vector_db["client"]
            result = es.index(
                index=self.collection_name,
                document={
                    "embedding": embedding,
                    "thought": thought_dict
                }
            )
            return result["_id"]

    def search_similar(self, query: str, limit: int = 5) -> List[Thought]:
        """Search for thoughts similar to the query.

        Args:
            query: The query to search for.
            limit: Maximum number of results to return.

        Returns:
            A list of thoughts similar to the query.
        """
        # Generate embedding for the query
        embedding = self.embedding_model.embed(query)

        # Search in vector database
        if self.vector_db["type"] == "milvus":
            from pymilvus import Collection
            collection = Collection(self.collection_name)
            search_params = {"metric_type": "COSINE", "params": {"nprobe": 10}}
            results = collection.search(
                data=[embedding],
                anns_field="embedding",
                param=search_params,
                limit=limit,
                output_fields=["json"]
            )

            thoughts = []
            for hits in results:
                for hit in hits:
                    thought_dict = json.loads(hit.entity.get("json"))
                    thoughts.append(Thought.from_dict(thought_dict))
            return thoughts
        elif self.vector_db["type"] == "elasticsearch":
            es = self.vector_db["client"]
            results = es.search(
                index=self.collection_name,
                knn={
                    "field": "embedding",
                    "query_vector": embedding,
                    "k": limit,
                    "num_candidates": 100
                }
            )

            thoughts = []
            for hit in results["hits"]["hits"]:
                thought_dict = hit["_source"]["thought"]
                thoughts.append(Thought.from_dict(thought_dict))
            return thoughts
```

### Hybrid Memory Architecture

For the most flexible and powerful memory system, a hybrid approach combining multiple storage mechanisms is recommended:

1. **Short-term Memory**: Redis for fast access to recent Thoughts
2. **Long-term Memory**: Vector database for semantic search across all historical Thoughts
3. **Episodic Memory**: File-based storage for complete conversation histories
4. **Working Memory**: In-memory cache for the current conversation context

This architecture would allow for sophisticated memory management strategies, such as:

- Automatically moving Thoughts from short-term to long-term memory after a certain time
- Retrieving relevant past Thoughts based on semantic similarity to the current context
- Maintaining different types of memory for different purposes (factual knowledge vs. conversational context)
- Implementing forgetting mechanisms to prevent memory overload

## Benefits

1. **Improved Context Sharing**: All components have access to the complete context, including retrieved documents, validation results, and critic feedback.
2. **Better Traceability**: Full history of the generation process is preserved, making it easier to debug and analyze.
3. **Enhanced Persistence**: State can be saved and loaded for resuming or analysis, enabling more complex workflows.
4. **Flexible Memory Models**: Different types of memory can be implemented by extending the Thought container.
5. **Richer Feedback Loop**: Critics have access to validation details and history, enabling more sophisticated improvement strategies.
6. **Retrieval-Augmented Generation**: Models can use retrieved documents to generate more informed responses.
7. **Semantic Memory**: Vector databases enable finding relevant past experiences based on meaning.

## Potential Challenges

1. **Performance Overhead**: Managing the state container adds some overhead, especially with large histories.
2. **Complexity**: More complex architecture may be harder to understand and maintain.
3. **Memory Usage**: Storing complete history could use significant memory for long chains.
4. **API Design**: Designing a clean API that supports both the new and old approaches.
5. **Database Management**: Additional infrastructure requirements for Redis/Milvus/Elasticsearch.

## Next Steps

1. Implement the `Thought` container in `sifaka/state.py`
2. Create unit tests for the `Thought` container
3. Implement the `RetrievalAugmentedModel` wrapper
4. Update the `Chain` class to support retrievers and the `Thought` container
5. Implement basic file-based persistence for Thoughts
6. Create examples demonstrating the new capabilities
7. Implement advanced memory stores (Redis, Milvus, Elasticsearch)
8. Develop memory management strategies for different use cases

## Is This a Good Idea? Evaluating the Approach

### Advantages

1. **Unified State Management**: The state container provides a single source of truth for the entire generation process, making it easier to reason about and debug.

2. **Enhanced Capabilities**: Making retrievers available to models enables more powerful generation capabilities, especially for knowledge-intensive tasks.

3. **Flexible Memory Models**: The ability to persist and retrieve state enables sophisticated memory management, which is essential for building agents with memory.

4. **Clean Architecture**: Starting with a clean rewrite allows us to implement a more coherent and maintainable architecture without the constraints of backward compatibility.

5. **Future-Proofing**: The design is flexible enough to accommodate future enhancements, such as new types of retrievers, validators, or critics.

### Potential Drawbacks

1. **Increased Complexity**: The new architecture is more complex than the current one, which could make it harder for new users to understand.

2. **Performance Overhead**: Managing state and persistence adds some overhead, which could impact performance for simple use cases.

3. **Implementation Effort**: A complete rewrite requires significant effort, especially to implement all the advanced memory features.

### Naming Considerations

The name of the state container is important as it shapes how users think about and interact with it. Let's evaluate the options:

1. **Thought**:
   - **Pros**: Intuitive for representing a cognitive process, aligns with the idea of "thinking" through a problem
   - **Cons**: May imply a more human-like process than what's actually happening, could be confused with other AI concepts

2. **Assertion**:
   - **Pros**: Emphasizes the factual nature of the generated content, clear connection to validation
   - **Cons**: Too narrow, doesn't capture the iterative improvement aspect well

3. **Discussion**:
   - **Pros**: Highlights the back-and-forth nature of the process, good for conversational contexts
   - **Cons**: May not fit well for non-conversational use cases

4. **Context**:
   - **Pros**: Directly describes its role in maintaining context, technically accurate
   - **Cons**: Too generic, already used in many other contexts in programming

5. **State**:
   - **Pros**: Clear, technical, and accurate description of its purpose
   - **Cons**: Very generic, doesn't convey the specific nature of what it contains

6. **Memory**:
   - **Pros**: Aligns well with the persistence aspect, intuitive for users
   - **Cons**: Could be confused with computer memory or other memory concepts

7. **Reflection**:
   - **Pros**: Captures the iterative, self-improving nature of the process
   - **Cons**: May imply more introspection than what's actually happening

**Recommendation**: "Thought" seems to be the most intuitive and descriptive option, but "Context" or "State" might be more technically accurate. The final choice should align with how users are expected to think about and interact with the system.

## Example Usage Patterns

### Basic Usage with Retrieval-Augmented Generation

```python
from sifaka import Chain
from sifaka.retrievers import SimpleRetriever

# Create a retriever with some documents
documents = [
    "Sifaka is a framework for text generation and improvement.",
    "Retrievers are used to find relevant information for a query.",
    "Critics improve text quality through various techniques."
]
retriever = SimpleRetriever(documents)

# Create a chain with retrieval-augmented generation
chain = (Chain()
    .with_model("openai:gpt-4")
    .with_prompt("What is Sifaka and how does it use retrievers?")
    .with_retriever(retriever)
    .validate_with(length(min_words=50, max_words=500))
    .run())

print(chain.text)
```

### Accessing the Thought Container

```python
from sifaka import Chain
from sifaka.state import Thought
from sifaka.retrievers import SimpleRetriever

# Create a retriever with some documents
documents = [
    "Sifaka is a framework for text generation and improvement.",
    "Retrievers are used to find relevant information for a query.",
    "Critics improve text quality through various techniques."
]
retriever = SimpleRetriever(documents)

# Create a chain with retrieval-augmented generation
chain = Chain()
chain.with_model("openai:gpt-4")
chain.with_prompt("What is Sifaka and how does it use retrievers?")
chain.with_retriever(retriever)
chain.validate_with(length(min_words=50, max_words=500))

# Run the chain and get the result
result = chain.run()

# Access the thought container from the result
thought = result.metadata.get("thought")

# Print the retrieved context
print("Retrieved context:")
for i, doc in enumerate(thought.retrieved_context):
    print(f"[{i+1}] {doc}")

# Print validation results
print("\nValidation results:")
for i, validation in enumerate(thought.validation_results):
    print(f"Validation {i+1}: {'Passed' if validation.passed else 'Failed'}")
    if validation.issues:
        print(f"  Issues: {', '.join(validation.issues)}")
    if validation.suggestions:
        print(f"  Suggestions: {', '.join(validation.suggestions)}")

# Save the thought to a file for later analysis
thought.save_to_file("thought.json")
```

### Creating an Agent with Memory

```python
from sifaka import Chain
from sifaka.state import Thought
from sifaka.retrievers import SimpleRetriever
import os

class Agent:
    """Simple agent with memory using the Thought container."""

    def __init__(self, model_name: str, memory_dir: str = "memory"):
        """Initialize the agent.

        Args:
            model_name: The name of the model to use.
            memory_dir: The directory to store memory files.
        """
        self.model_name = model_name
        self.memory_dir = memory_dir
        self.thoughts = []

        # Create memory directory if it doesn't exist
        os.makedirs(memory_dir, exist_ok=True)

        # Load existing memories
        self._load_memories()

    def _load_memories(self):
        """Load memories from disk."""
        for filename in os.listdir(self.memory_dir):
            if filename.endswith(".json"):
                file_path = os.path.join(self.memory_dir, filename)
                thought = Thought.load_from_file(file_path)
                self.thoughts.append(thought)

    def answer(self, prompt: str) -> str:
        """Answer a question using the agent's memory.

        Args:
            prompt: The question to answer.

        Returns:
            The agent's response.
        """
        # Create a retriever from the agent's memories
        documents = []
        for thought in self.thoughts:
            if thought.generated_text:
                documents.append(thought.generated_text)

        retriever = SimpleRetriever(documents)

        # Create a chain with retrieval-augmented generation
        chain = (Chain()
            .with_model(self.model_name)
            .with_prompt(prompt)
            .with_retriever(retriever)
            .run())

        # Save the new thought to memory
        thought = chain.metadata.get("thought")
        if thought:
            self.thoughts.append(thought)
            thought.save_to_file(os.path.join(self.memory_dir, f"thought_{len(self.thoughts)}.json"))

        return chain.text

# Create an agent
agent = Agent(model_name="openai:gpt-4")

# Ask the agent questions
response1 = agent.answer("What is Sifaka?")
print(response1)

response2 = agent.answer("How does Sifaka use retrievers?")
print(response2)

response3 = agent.answer("What did I ask you earlier?")
print(response3)
```

### Using the Thought Container with Custom Components

```python
from sifaka.state import Thought
from sifaka.models import create_model
from sifaka.retrievers import SimpleRetriever
from typing import List, Dict, Any

class CustomAgent:
    """Custom agent that uses the Thought container directly."""

    def __init__(self, model_name: str):
        """Initialize the agent.

        Args:
            model_name: The name of the model to use.
        """
        self.model = create_model(model_name)
        self.documents = []
        self.retriever = SimpleRetriever(self.documents)

    def add_document(self, document: str):
        """Add a document to the agent's knowledge.

        Args:
            document: The document to add.
        """
        self.documents.append(document)
        # Update the retriever with the new documents
        self.retriever = SimpleRetriever(self.documents)

    def process(self, prompt: str) -> Thought:
        """Process a prompt and return a Thought.

        Args:
            prompt: The prompt to process.

        Returns:
            A Thought container with the processing results.
        """
        # Create a new Thought
        thought = Thought(prompt=prompt)

        # Retrieve relevant documents
        thought.retrieved_context = self.retriever.retrieve(prompt)

        # Generate text
        if thought.retrieved_context:
            # Create augmented prompt with retrieved context
            augmented_prompt = f"{prompt}\n\nRelevant information:\n"
            for i, doc in enumerate(thought.retrieved_context):
                augmented_prompt += f"[{i+1}] {doc}\n\n"

            thought.generated_text = self.model.generate(augmented_prompt)
        else:
            thought.generated_text = self.model.generate(prompt)

        return thought

# Create a custom agent
agent = CustomAgent(model_name="openai:gpt-4")

# Add some documents to the agent's knowledge
agent.add_document("Sifaka is a framework for text generation and improvement.")
agent.add_document("Retrievers are used to find relevant information for a query.")
agent.add_document("Critics improve text quality through various techniques.")

# Process a prompt
thought = agent.process("What is Sifaka and how does it use retrievers?")

# Print the results
print(f"Prompt: {thought.prompt}")
print(f"Retrieved context: {thought.retrieved_context}")
print(f"Generated text: {thought.generated_text}")

# Save the thought to a file
thought.save_to_file("custom_agent_thought.json")
```
